{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGIb5rED554x",
        "outputId": "a393401a-2d10-4344-ee55-2f8a3949949e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount= True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "\n",
        "\n",
        "dataset = keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=\"/content/drive/MyDrive/GAN Image to Image Translation/disastermanage/DamageClassify/no_damage\", label_mode=None, image_size=(128, 128), batch_size=32,\n",
        "    shuffle=True, seed=None, validation_split=None,\n",
        ").map(lambda x: x / 255.0*2.0 - 1.0)\n",
        "\n",
        "print(dataset)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(128, 128, 3)),\n",
        "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(0.2),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(0.2),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")\n",
        "\n",
        "\n",
        "latent_dim = 128\n",
        "generator = keras.Sequential([\n",
        "\n",
        "    layers.Input(shape=(latent_dim,)),\n",
        "    layers.Dense(8 * 8 * 128),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.LeakyReLU(0.2),\n",
        "    layers.Reshape((8, 8, 128)),\n",
        "    layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.LeakyReLU(0.2),\n",
        "    layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.LeakyReLU(0.2),\n",
        "    layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.LeakyReLU(0.2),\n",
        "    layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),  # Output shape: (128, 128, 512)\n",
        "    layers.BatchNormalization(),\n",
        "    layers.LeakyReLU(0.2),\n",
        "    layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"tanh\")\n",
        "\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "opt_gen = keras.optimizers.Adam(1e-4)\n",
        "opt_disc = keras.optimizers.Adam(1e-4)\n",
        "loss_fn = keras.losses.BinaryCrossentropy()\n",
        "save_directory = \"/content/drive/MyDrive/GAN Image to Image Translation/disastermanage/models\"\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "if not os.path.exists(save_directory):\n",
        "    os.makedirs(save_directory)\n",
        "\n",
        "for epoch in range(50):\n",
        "    for idx, real in enumerate(tqdm(dataset)):\n",
        "        batch_size = real.shape[0]\n",
        "\n",
        "        random_latent_vectors = tf.random.normal(shape = (batch_size, latent_dim))\n",
        "        fake = generator(random_latent_vectors)\n",
        "\n",
        "        if idx % 100 == 0:\n",
        "            img = keras.preprocessing.image.array_to_img((fake[0] + 1) * 0.5)\n",
        "            img.save(f\"/content/drive/MyDrive/GAN Image to Image Translation/disastermanage/generated_images/generated_img_{epoch}_{idx}.png\")\n",
        "\n",
        "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
        "        with tf.GradientTape() as disc_tape:\n",
        "            loss_disc_real = loss_fn(tf.ones((batch_size, 1)), discriminator(real))\n",
        "            loss_disc_fake = loss_fn(tf.zeros((batch_size, 1)), discriminator(fake))\n",
        "            loss_disc = (loss_disc_real + loss_disc_fake)/2\n",
        "\n",
        "        grads = disc_tape.gradient(loss_disc, discriminator.trainable_weights)\n",
        "        opt_disc.apply_gradients(zip(grads, discriminator.trainable_weights))\n",
        "\n",
        "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
        "        with tf.GradientTape() as gen_tape:\n",
        "            fake = generator(random_latent_vectors)\n",
        "            output = discriminator(fake)\n",
        "            loss_gen = loss_fn(tf.ones(batch_size, 1), output)\n",
        "\n",
        "        grads = gen_tape.gradient(loss_gen, generator.trainable_weights)\n",
        "        opt_gen.apply_gradients(zip(grads, generator.trainable_weights))\n",
        "\n",
        "generator.save(os.path.join(save_directory, \"generator_final.h5\"))\n",
        "discriminator.save(os.path.join(save_directory, \"discriminator_final.h5\"))\n"
      
